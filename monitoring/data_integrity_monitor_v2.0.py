#!/usr/bin/env python3
"""
æ•°æ®çœŸå®æ€§ç›‘æµ‹å·¥å…· v2.0 - å®æ—¶ç›‘æ§ç‰ˆ
Data Integrity Monitor v2.0 for Investment Research Reports

æ–°åŠŸèƒ½:
1. å®æ—¶æ–‡ä»¶ç›‘æ§ï¼ˆwatchdogï¼‰
2. è‡ªåŠ¨è´¨é‡è¯„åˆ†v2.0ç®—æ³•
3. å®æ—¶è´¨é‡ä»ªè¡¨ç›˜æ›´æ–°
4. è‡ªåŠ¨ä¿®æ­£å»ºè®®ç”Ÿæˆ
5. è´¨é‡<50åˆ†è‡ªåŠ¨å‘Šè­¦
6. æ•°æ®æ¥æºè‡ªåŠ¨åˆ†ç±»ï¼ˆTier 1/2/3ï¼‰
7. æ•°æ®æ—¶æ•ˆæ€§è¯„åˆ†

ä½¿ç”¨æ–¹æ³•:
python data_integrity_monitor_v2.0.py <æŠ¥å‘Šæ–‡ä»¶è·¯å¾„>  # å•æ¬¡æ‰«æ
python data_integrity_monitor_v2.0.py --watch  # å®æ—¶ç›‘æ§æ¨¡å¼
python data_integrity_monitor_v2.0.py --watch --auto-update-dashboard  # ç›‘æ§+ä»ªè¡¨ç›˜
"""

import re
import sys
import os
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict

try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdogæœªå®‰è£…ï¼Œå®æ—¶ç›‘æ§åŠŸèƒ½ä¸å¯ç”¨")
    print("å®‰è£…: pip install watchdog")


@dataclass
class DataPoint:
    """æ•°æ®ç‚¹å¯¹è±¡"""
    value: str
    context: str
    line_number: int
    data_type: str
    has_source: bool
    source_tier: str  # NEW: Tier 1/2/3
    label: str
    confidence: int
    timestamp: str  # NEW: æ•°æ®æ—¶é—´æˆ³


@dataclass
class QualityReport:
    """æ•°æ®è´¨é‡æŠ¥å‘Šv2.0"""
    file_name: str
    file_path: str
    scan_time: str
    total_data_points: int
    verified_count: int
    estimated_count: int
    sample_count: int
    unverified_count: int
    source_tier1_count: int  # NEW
    source_tier2_count: int  # NEW
    source_tier3_count: int  # NEW
    quality_score: float
    quality_grade: str  # NEW: A+/B/C/F
    verifiable_data_ratio: float  # NEW
    source_annotation_ratio: float  # NEW
    timeliness_score: float  # NEW
    logic_completeness_score: float  # NEW
    disclaimer_present: bool  # NEW
    issues: List[str]
    recommendations: List[str]
    auto_fix_suggestions: List[Dict]  # NEW


class DataIntegrityMonitorV2:
    """æ•°æ®å®Œæ•´æ€§ç›‘æµ‹å™¨ v2.0"""

    # æ•°æ®æ¨¡å¼
    PATTERNS = {
        'percentage': r'\b\d+\.?\d*%',
        'number_with_unit': r'\b\d+\.?\d*[KMBT](?:äº¿|ä¸‡|åƒ)?',
        'currency': r'\$\d+(?:,\d{3})*(?:\.\d+)?[KMBT]?',
        'plain_number': r'\b\d{1,3}(?:,\d{3})+(?:\.\d+)?',
        'decimal': r'\b\d+\.\d+(?!%)',
        'date': r'\b20\d{2}[-/]\d{1,2}[-/]\d{1,2}\b',
        'time': r'\b\d{1,2}:\d{2}(?::\d{2})?\s?(?:UTC|EST|PST)?\b',
    }

    # æ ‡æ³¨ç¬¦å·
    LABELS = {
        'âœ…': 'VERIFIED',
        'ğŸ“Š': 'ESTIMATED',
        'ğŸ¯': 'MODELED',
        'âš ï¸': 'SAMPLE',
        'âŒ': 'UNVERIFIED',
        '[VERIFIED]': 'VERIFIED',
        '[ESTIMATED]': 'ESTIMATED',
        '[SAMPLE]': 'SAMPLE',
    }

    # æ•°æ®æ¥æºåˆ†å±‚ï¼ˆNEWï¼‰
    SOURCE_TIER1 = [
        'SEC', 'EDGAR', '10-K', '10-Q', '8-K', 'è´¢æŠ¥', 'earnings report',
        'investor relations', 'IR.', 'å®˜æ–¹å‘å¸ƒ', 'official release',
        'press release', 'æ–°é—»ç¨¿', 'annual report', 'å¹´æŠ¥',
        'å­£æŠ¥', 'quarterly report'
    ]

    SOURCE_TIER2 = [
        'Bloomberg', 'Reuters', 'Yahoo Finance', 'FactSet', 'S&P Capital IQ',
        'Morningstar', 'Seeking Alpha', 'MarketWatch', 'CNBC',
        'Wall Street Journal', 'Financial Times',
        'çŸ¥ååˆ†æå¸ˆ', 'top analyst', 'è¡Œä¸šåä¼š', 'industry association',
        'IDC', 'Gartner', 'McKinsey', 'BCG', 'Bain'
    ]

    SOURCE_TIER3 = [
        'Twitter', 'Reddit', 'social media', 'ç¤¾äº¤åª’ä½“',
        'rumor', 'ä¼ é—»', 'æ®æ‚‰', 'æ¶ˆæ¯äººå£«',
        'anonymous', 'åŒ¿å', 'unconfirmed', 'æœªè¯å®',
        'allegedly', 'æ®ç§°'
    ]

    # å¯ç–‘å…³é”®è¯ï¼ˆè¡¨æ˜è™šæ„æ•°æ®ï¼‰
    SUSPICIOUS_KEYWORDS = [
        'Engine 1', 'Engine 2', 'Engine 3', 'Engine 4', 'Engine 5', 'Engine 6',
        'å®æ—¶ç›‘æ§', 'real-time monitoring',
        'å¼‚å¸¸å¤§å•', 'unusual activity',
        'å†…éƒ¨æ¶ˆæ¯', 'inside information',
        'ç‹¬å®¶è·æ‚‰', 'exclusively learned',
        'æš—æ± æ•°æ®', 'dark pool data',
        'ç‹¬å®¶ç®—æ³•', 'proprietary algorithm'
    ]

    # éœ€è¦æ¥æºçš„å…³é”®æ•°æ®
    REQUIRE_SOURCE = [
        'æ¯›åˆ©ç‡', 'äº¤ä»˜é‡', 'å¸‚å€¼', 'è¥æ”¶', 'åˆ©æ¶¦', 'å¢é•¿ç‡',
        'margin', 'delivery', 'revenue', 'profit', 'market cap',
        'growth rate', 'EBITDA', 'EPS', 'P/E', 'ROE', 'ROIC'
    ]

    def __init__(self):
        self.data_points: List[DataPoint] = []
        self.issues: List[str] = []
        self.recommendations: List[str] = []
        self.auto_fix_suggestions: List[Dict] = []
        self.all_reports: List[QualityReport] = []

    def scan_file(self, file_path: str) -> QualityReport:
        """æ‰«æå•ä¸ªæ–‡ä»¶"""
        print(f"\n{'='*80}")
        print(f"æ‰«ææ–‡ä»¶: {file_path}")
        print(f"æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")

        self.data_points = []
        self.issues = []
        self.recommendations = []
        self.auto_fix_suggestions = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return None

        # é€è¡Œæ‰«æ
        for i, line in enumerate(lines, 1):
            self._scan_line(line, i, file_path)

        # æ£€æŸ¥å…è´£å£°æ˜
        content = ''.join(lines)
        has_disclaimer = self._check_disclaimer(content)

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report_v2(file_path, has_disclaimer)
        return report

    def _scan_line(self, line: str, line_number: int, file_path: str):
        """æ‰«æå•è¡Œ"""
        # æ£€æµ‹æ•°å­—
        for pattern_name, pattern in self.PATTERNS.items():
            matches = re.finditer(pattern, line)
            for match in matches:
                value = match.group()
                context = line.strip()

                # åˆ¤æ–­æ ‡æ³¨ç±»å‹
                label = self._detect_label(line)

                # åˆ¤æ–­æ•°æ®æ¥æºåˆ†å±‚
                source_tier, has_source = self._detect_source_tier(line, context)

                # æå–æ—¶é—´æˆ³
                timestamp = self._extract_timestamp(line, context)

                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = self._calculate_confidence_v2(
                    value, context, label, source_tier, timestamp
                )

                data_point = DataPoint(
                    value=value,
                    context=context,
                    line_number=line_number,
                    data_type=pattern_name,
                    has_source=has_source,
                    source_tier=source_tier,
                    label=label,
                    confidence=confidence,
                    timestamp=timestamp
                )

                self.data_points.append(data_point)

                # ç”Ÿæˆè‡ªåŠ¨ä¿®æ­£å»ºè®®
                if confidence < 50:
                    self._generate_auto_fix(data_point, file_path)

        # æ£€æµ‹å¯ç–‘å…³é”®è¯
        for keyword in self.SUSPICIOUS_KEYWORDS:
            if keyword in line:
                self.issues.append(
                    f"âš ï¸ è¡Œ{line_number}: å‘ç°å¯ç–‘å…³é”®è¯ '{keyword}' - {line.strip()[:80]}"
                )
                self.auto_fix_suggestions.append({
                    'line': line_number,
                    'issue': f"å¯ç–‘å…³é”®è¯: {keyword}",
                    'original': line.strip()[:100],
                    'suggestion': f"å»ºè®®åˆ é™¤æˆ–é‡å†™æ­¤è¡Œï¼Œé¿å…ä½¿ç”¨'{keyword}'ç­‰ä¸å¯éªŒè¯çš„æ¥æº",
                    'severity': 'HIGH'
                })

        # æ£€æµ‹ç¼ºå°‘æ¥æºçš„å…³é”®æ•°æ®
        for keyword in self.REQUIRE_SOURCE:
            if keyword in line.lower():
                _, has_source = self._detect_source_tier(line, line)
                if not has_source:
                    self.issues.append(
                        f"âŒ è¡Œ{line_number}: '{keyword}' æ•°æ®ç¼ºå°‘æ¥æºæ ‡æ³¨ - {line.strip()[:80]}"
                    )
                    self._generate_source_suggestion(line, line_number, keyword)

    def _detect_label(self, line: str) -> str:
        """æ£€æµ‹æ•°æ®æ ‡æ³¨ç±»å‹"""
        for emoji, label in self.LABELS.items():
            if emoji in line:
                return label

        if 'VERIFIED' in line or 'æ¥æº:' in line or 'Source:' in line:
            return 'VERIFIED'
        elif 'ESTIMATED' in line or 'ä¼°ç®—' in line or 'é¢„æµ‹' in line:
            return 'ESTIMATED'
        elif 'SAMPLE' in line or 'EXAMPLE' in line or 'ç¤ºä¾‹' in line or 'æ¼”ç¤º' in line:
            return 'SAMPLE'
        elif 'Engine' in line:
            return 'UNVERIFIED'

        return 'UNLABELED'

    def _detect_source_tier(self, line: str, context: str) -> Tuple[str, bool]:
        """æ£€æµ‹æ•°æ®æ¥æºåˆ†å±‚ï¼ˆNEWï¼‰"""
        # Tier 1: å®˜æ–¹æ¥æº
        for indicator in self.SOURCE_TIER1:
            if indicator in line or indicator in context:
                return 'Tier 1', True

        # Tier 2: çŸ¥åæœºæ„
        for indicator in self.SOURCE_TIER2:
            if indicator in line or indicator in context:
                return 'Tier 2', True

        # Tier 3: ç¤¾äº¤åª’ä½“/ä¼ é—»
        for indicator in self.SOURCE_TIER3:
            if indicator in line or indicator in context:
                return 'Tier 3', True

        return 'No Source', False

    def _extract_timestamp(self, line: str, context: str) -> str:
        """æå–æ•°æ®æ—¶é—´æˆ³ï¼ˆNEWï¼‰"""
        # æŸ¥æ‰¾æ—¥æœŸ
        date_pattern = r'\b(20\d{2})[-/å¹´](\d{1,2})[-/æœˆ](\d{1,2})[æ—¥]?\b'
        match = re.search(date_pattern, context)
        if match:
            return f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"

        # æŸ¥æ‰¾å­£åº¦
        quarter_pattern = r'\b(20\d{2})\s*Q([1-4])\b'
        match = re.search(quarter_pattern, context, re.IGNORECASE)
        if match:
            return f"{match.group(1)}-Q{match.group(2)}"

        # æŸ¥æ‰¾å¹´ä»½
        year_pattern = r'\b(20\d{2})å¹´?\b'
        match = re.search(year_pattern, context)
        if match:
            return match.group(1)

        return 'Unknown'

    def _calculate_confidence_v2(self, value: str, context: str,
                                 label: str, source_tier: str,
                                 timestamp: str) -> int:
        """è®¡ç®—æ•°æ®ç½®ä¿¡åº¦v2.0ï¼ˆNEWï¼‰"""
        score = 50  # åŸºç¡€åˆ†

        # æ ‡æ³¨ç±»å‹åŠ åˆ†
        if label == 'VERIFIED':
            score += 30
        elif label == 'ESTIMATED':
            score += 10
        elif label == 'SAMPLE':
            score -= 20
        elif label == 'UNVERIFIED':
            score -= 40

        # æ¥æºåˆ†å±‚åŠ åˆ†ï¼ˆNEWï¼‰
        if source_tier == 'Tier 1':
            score += 30
        elif source_tier == 'Tier 2':
            score += 15
        elif source_tier == 'Tier 3':
            score -= 20
        else:
            score -= 15

        # æ—¶æ•ˆæ€§åŠ åˆ†ï¼ˆNEWï¼‰
        if timestamp != 'Unknown':
            try:
                # è§£ææ—¥æœŸ
                if 'Q' in timestamp:
                    year, quarter = timestamp.split('-Q')
                    data_date = datetime(int(year), int(quarter)*3, 1)
                elif len(timestamp) == 4:  # å¹´ä»½
                    data_date = datetime(int(timestamp), 1, 1)
                else:  # å®Œæ•´æ—¥æœŸ
                    data_date = datetime.strptime(timestamp, '%Y-%m-%d')

                days_old = (datetime.now() - data_date).days

                if days_old < 30:  # <1ä¸ªæœˆ
                    score += 15
                elif days_old < 90:  # <3ä¸ªæœˆ
                    score += 10
                elif days_old < 365:  # <1å¹´
                    score += 5
                else:  # >1å¹´
                    score -= 10
            except:
                pass

        # Engineç³»ç»Ÿæ•°æ®ä¸¥é‡æ‰£åˆ†
        if 'Engine' in context:
            score -= 50

        # å¯ç–‘å…³é”®è¯æ‰£åˆ†
        for keyword in self.SUSPICIOUS_KEYWORDS:
            if keyword in context:
                score -= 30
                break

        return max(0, min(100, score))

    def _check_disclaimer(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å…è´£å£°æ˜ï¼ˆNEWï¼‰"""
        disclaimer_keywords = [
            'å…è´£å£°æ˜', 'disclaimer', 'é£é™©æç¤º', 'risk warning',
            'ä¸æ„æˆæŠ•èµ„å»ºè®®', 'not investment advice',
            'for informational purposes only', 'ä»…ä¾›å‚è€ƒ'
        ]
        return any(keyword in content.lower() for keyword in disclaimer_keywords)

    def _generate_auto_fix(self, dp: DataPoint, file_path: str):
        """ç”Ÿæˆè‡ªåŠ¨ä¿®æ­£å»ºè®®ï¼ˆNEWï¼‰"""
        suggestion = {
            'file': os.path.basename(file_path),
            'line': dp.line_number,
            'issue': f"ç½®ä¿¡åº¦è¿‡ä½ ({dp.confidence}%)",
            'original': dp.context[:100],
            'data_point': dp.value,
            'current_label': dp.label,
            'current_source_tier': dp.source_tier,
            'severity': 'HIGH' if dp.confidence < 30 else 'MEDIUM'
        }

        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå…·ä½“å»ºè®®
        if dp.source_tier == 'No Source':
            suggestion['suggestion'] = (
                f"åŸæ–‡: \"{dp.context[:80]}...\"\n"
                f"é—®é¢˜: ç¼ºå°‘æ¥æºæ ‡æ³¨\n"
                f"å»ºè®®ä¿®æ­£: æ·»åŠ æ•°æ®æ¥æºï¼Œå¦‚ï¼š\n"
                f"  - [VERIFIED] (æ¥æº: TSLA 10-Q Q3 2025, p.15)\n"
                f"  - [ESTIMATED] (åŸºäºBloomberg Consensus)\n"
                f"  æˆ–åˆ é™¤æ­¤æ•°æ®ç‚¹"
            )
        elif dp.label == 'UNLABELED':
            suggestion['suggestion'] = (
                f"åŸæ–‡: \"{dp.context[:80]}...\"\n"
                f"é—®é¢˜: ç¼ºå°‘æ•°æ®æ ‡æ³¨\n"
                f"å»ºè®®ä¿®æ­£: æ·»åŠ æ ‡æ³¨ç¬¦å·ï¼š\n"
                f"  - [VERIFIED] - å¯éªŒè¯æ•°æ®\n"
                f"  - [ESTIMATED] - ä¼°ç®—æ•°æ®\n"
                f"  - [SAMPLE] - ç¤ºä¾‹æ•°æ®"
            )
        elif dp.source_tier == 'Tier 3':
            suggestion['suggestion'] = (
                f"åŸæ–‡: \"{dp.context[:80]}...\"\n"
                f"é—®é¢˜: ä½¿ç”¨äº†ä½å¯ä¿¡åº¦æ¥æº (Tier 3)\n"
                f"å»ºè®®ä¿®æ­£: \n"
                f"  - ç”¨Tier 1/2æ¥æºæ›¿æ¢\n"
                f"  - æˆ–æ ‡æ³¨ä¸º [ESTIMATED] å¹¶è¯´æ˜ä¸ç¡®å®šæ€§\n"
                f"  - æˆ–åˆ é™¤æ­¤æ•°æ®ç‚¹"
            )
        elif 'Engine' in dp.context:
            suggestion['suggestion'] = (
                f"åŸæ–‡: \"{dp.context[:80]}...\"\n"
                f"é—®é¢˜: åŒ…å«'Engine'å…³é”®è¯ï¼ˆå¯èƒ½ä¸ºè™šæ„æ•°æ®ï¼‰\n"
                f"å»ºè®®ä¿®æ­£: åˆ é™¤æ­¤è¡Œæˆ–ç”¨çœŸå®æ•°æ®æ›¿æ¢"
            )
        else:
            suggestion['suggestion'] = (
                f"åŸæ–‡: \"{dp.context[:80]}...\"\n"
                f"é—®é¢˜: ç»¼åˆç½®ä¿¡åº¦è¿‡ä½\n"
                f"å»ºè®®ä¿®æ­£: å¢åŠ æ¥æºæ ‡æ³¨æˆ–åˆ é™¤æ­¤æ•°æ®ç‚¹"
            )

        self.auto_fix_suggestions.append(suggestion)

    def _generate_source_suggestion(self, line: str, line_number: int, keyword: str):
        """ä¸ºç¼ºå°‘æ¥æºçš„å…³é”®æ•°æ®ç”Ÿæˆå»ºè®®ï¼ˆNEWï¼‰"""
        suggestion = {
            'line': line_number,
            'issue': f"å…³é”®æŒ‡æ ‡'{keyword}'ç¼ºå°‘æ¥æº",
            'original': line.strip()[:100],
            'suggestion': (
                f"æ£€æµ‹åˆ°å…³é”®æŒ‡æ ‡: {keyword}\n"
                f"åŸæ–‡: \"{line.strip()[:80]}...\"\n"
                f"å»ºè®®ä¿®æ­£: æ·»åŠ Tier 1æ¥æºï¼ˆSECæ–‡ä»¶/è´¢æŠ¥ï¼‰\n"
                f"ç¤ºä¾‹æ ¼å¼:\n"
                f"  - \"{line.strip()[:50]}...\" [VERIFIED] (æ¥æº: 10-Q Q4 2025, p.XX)\n"
                f"  - \"{line.strip()[:50]}...\" [ESTIMATED] (åŸºäºBloombergæ•°æ®)"
            ),
            'severity': 'HIGH'
        }
        self.auto_fix_suggestions.append(suggestion)

    def _calculate_quality_score_v2(self, report_data: Dict) -> Tuple[float, str]:
        """
        è´¨é‡è¯„åˆ†ç®—æ³•v2.0ï¼ˆNEWï¼‰

        è´¨é‡è¯„åˆ† = (
            å¯éªŒè¯æ•°æ®å æ¯” Ã— 40 +
            æ•°æ®æ¥æºæ ‡æ³¨ç‡ Ã— 30 +
            æ—¶æ•ˆæ€§å¾—åˆ† Ã— 15 +
            é€»è¾‘å®Œæ•´æ€§ Ã— 10 +
            å…è´£å£°æ˜ Ã— 5
        )
        """
        total = report_data['total_data_points']
        if total == 0:
            return 0.0, 'F'

        # 1. å¯éªŒè¯æ•°æ®å æ¯” (40åˆ†)
        verifiable = report_data['verified_count'] + report_data['estimated_count']
        verifiable_ratio = verifiable / total
        verifiable_score = verifiable_ratio * 40

        # 2. æ•°æ®æ¥æºæ ‡æ³¨ç‡ (30åˆ†)
        tier1 = report_data['source_tier1_count']
        tier2 = report_data['source_tier2_count']
        tier3 = report_data['source_tier3_count']
        source_ratio = (tier1 + tier2) / total  # Tier 3ä¸è®¡å…¥
        source_score = source_ratio * 30

        # 3. æ—¶æ•ˆæ€§å¾—åˆ† (15åˆ†)
        # è®¡ç®—æœ‰æ—¶é—´æˆ³çš„æ•°æ®å æ¯”
        timestamped = sum(1 for dp in self.data_points if dp.timestamp != 'Unknown')
        timeliness_ratio = timestamped / total if total > 0 else 0
        timeliness_score = timeliness_ratio * 15

        # 4. é€»è¾‘å®Œæ•´æ€§ (10åˆ†)
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜
        serious_issues = len([i for i in self.issues if 'âŒ' in i])
        logic_score = max(0, 10 - serious_issues * 2)

        # 5. å…è´£å£°æ˜ (5åˆ†)
        disclaimer_score = 5 if report_data['has_disclaimer'] else 0

        # æ€»åˆ†
        total_score = (
            verifiable_score +
            source_score +
            timeliness_score +
            logic_score +
            disclaimer_score
        )

        # è¯„çº§
        if total_score >= 90:
            grade = 'A+'
        elif total_score >= 70:
            grade = 'B'
        elif total_score >= 50:
            grade = 'C'
        else:
            grade = 'F'

        return round(total_score, 2), grade

    def _generate_report_v2(self, file_path: str, has_disclaimer: bool) -> QualityReport:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Šv2.0ï¼ˆNEWï¼‰"""
        # ç»Ÿè®¡å„ç±»æ•°æ®
        verified = sum(1 for dp in self.data_points if dp.label == 'VERIFIED')
        estimated = sum(1 for dp in self.data_points if dp.label == 'ESTIMATED')
        sample = sum(1 for dp in self.data_points if dp.label == 'SAMPLE')
        unverified = sum(1 for dp in self.data_points
                        if dp.label in ['UNVERIFIED', 'UNLABELED'])

        # ç»Ÿè®¡æ¥æºåˆ†å±‚
        tier1 = sum(1 for dp in self.data_points if dp.source_tier == 'Tier 1')
        tier2 = sum(1 for dp in self.data_points if dp.source_tier == 'Tier 2')
        tier3 = sum(1 for dp in self.data_points if dp.source_tier == 'Tier 3')

        total = len(self.data_points)

        # è®¡ç®—å„é¡¹æ¯”ç‡
        verifiable_ratio = (verified + estimated) / total if total > 0 else 0
        source_annotation_ratio = (tier1 + tier2 + tier3) / total if total > 0 else 0

        # æ—¶æ•ˆæ€§è¯„åˆ†
        recent_data = sum(1 for dp in self.data_points
                         if dp.timestamp != 'Unknown' and
                         self._is_recent(dp.timestamp))
        timeliness_score = (recent_data / total * 100) if total > 0 else 0

        # é€»è¾‘å®Œæ•´æ€§è¯„åˆ†
        serious_issues = len([i for i in self.issues if 'âŒ' in i])
        logic_score = max(0, 100 - serious_issues * 10)

        # å‡†å¤‡æ•°æ®ç”¨äºè´¨é‡è¯„åˆ†
        report_data = {
            'total_data_points': total,
            'verified_count': verified,
            'estimated_count': estimated,
            'source_tier1_count': tier1,
            'source_tier2_count': tier2,
            'source_tier3_count': tier3,
            'has_disclaimer': has_disclaimer
        }

        # è®¡ç®—è´¨é‡åˆ†æ•°v2.0
        quality_score, quality_grade = self._calculate_quality_score_v2(report_data)

        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations_v2(
            quality_score, quality_grade, verifiable_ratio,
            source_annotation_ratio, tier3, total, has_disclaimer
        )

        report = QualityReport(
            file_name=os.path.basename(file_path),
            file_path=file_path,
            scan_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            total_data_points=total,
            verified_count=verified,
            estimated_count=estimated,
            sample_count=sample,
            unverified_count=unverified,
            source_tier1_count=tier1,
            source_tier2_count=tier2,
            source_tier3_count=tier3,
            quality_score=quality_score,
            quality_grade=quality_grade,
            verifiable_data_ratio=round(verifiable_ratio * 100, 2),
            source_annotation_ratio=round(source_annotation_ratio * 100, 2),
            timeliness_score=round(timeliness_score, 2),
            logic_completeness_score=round(logic_score, 2),
            disclaimer_present=has_disclaimer,
            issues=self.issues,
            recommendations=self.recommendations,
            auto_fix_suggestions=self.auto_fix_suggestions
        )

        return report

    def _is_recent(self, timestamp: str) -> bool:
        """åˆ¤æ–­æ•°æ®æ˜¯å¦ä¸ºè¿‘æœŸï¼ˆ6ä¸ªæœˆå†…ï¼‰"""
        try:
            if 'Q' in timestamp:
                year, quarter = timestamp.split('-Q')
                data_date = datetime(int(year), int(quarter)*3, 1)
            elif len(timestamp) == 4:
                data_date = datetime(int(timestamp), 1, 1)
            else:
                data_date = datetime.strptime(timestamp, '%Y-%m-%d')

            days_old = (datetime.now() - data_date).days
            return days_old < 180
        except:
            return False

    def _generate_recommendations_v2(self, quality_score: float, grade: str,
                                    verifiable_ratio: float,
                                    source_ratio: float,
                                    tier3_count: int, total: int, has_disclaimer: bool):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®v2.0ï¼ˆNEWï¼‰"""
        # æ€»ä½“è¯„ä»·
        if grade == 'F':
            self.recommendations.append(
                f"ğŸ”´ è´¨é‡è¯„åˆ†: {quality_score}/100 (Fçº§ - ä¸åˆæ ¼)\n"
                "   å»ºè®®: å…¨é¢ä¿®è®¢æ•°æ®æ¥æºï¼Œåˆ é™¤æ‰€æœ‰ä¸å¯éªŒè¯æ•°æ®"
            )
        elif grade == 'C':
            self.recommendations.append(
                f"ğŸŸ¡ è´¨é‡è¯„åˆ†: {quality_score}/100 (Cçº§ - éœ€å¤§ä¿®)\n"
                "   å»ºè®®: è¡¥å……æ•°æ®æ¥æºæ ‡æ³¨ï¼Œæ›¿æ¢ä½è´¨é‡æ•°æ®"
            )
        elif grade == 'B':
            self.recommendations.append(
                f"ğŸŸ¢ è´¨é‡è¯„åˆ†: {quality_score}/100 (Bçº§ - å¯ç”¨)\n"
                "   å»ºè®®: å°å¹…ä¼˜åŒ–æ•°æ®æ¥æºï¼Œå¯è€ƒè™‘å‘å¸ƒ"
            )
        else:  # A+
            self.recommendations.append(
                f"âœ… è´¨é‡è¯„åˆ†: {quality_score}/100 (A+çº§ - å‘å¸ƒçº§)\n"
                "   å»ºè®®: æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œè¾¾åˆ°å‘å¸ƒæ ‡å‡†"
            )

        # å¯éªŒè¯æ•°æ®æ¯”ä¾‹
        if verifiable_ratio < 0.3:
            self.recommendations.append(
                f"âŒ å¯éªŒè¯æ•°æ®è¿‡å°‘ ({verifiable_ratio*100:.1f}%)\n"
                "   å»ºè®®: è‡³å°‘30%æ•°æ®åº”æ ‡æ³¨ä¸º[VERIFIED]æˆ–[ESTIMATED]"
            )

        # æ¥æºæ ‡æ³¨æ¯”ä¾‹
        if source_ratio < 0.5:
            self.recommendations.append(
                f"âš ï¸ æ•°æ®æ¥æºæ ‡æ³¨ä¸è¶³ ({source_ratio*100:.1f}%)\n"
                "   å»ºè®®: è‡³å°‘50%æ•°æ®åº”æ ‡æ³¨Tier 1æˆ–Tier 2æ¥æº"
            )

        # Tier 3æ¥æºè¿‡å¤š
        if tier3_count > total * 0.1:
            self.recommendations.append(
                f"ğŸ”´ Tier 3ä½è´¨é‡æ¥æºè¿‡å¤š ({tier3_count}/{total})\n"
                "   å»ºè®®: ç”¨Tier 1(SEC/è´¢æŠ¥)æˆ–Tier 2(Bloomberg)æ¥æºæ›¿æ¢"
            )

        # æ—¶æ•ˆæ€§
        if self.data_points:
            old_data = sum(1 for dp in self.data_points
                          if dp.timestamp != 'Unknown' and
                          not self._is_recent(dp.timestamp))
            if old_data > total * 0.3:
                self.recommendations.append(
                    f"âš ï¸ è¿‡æœŸæ•°æ®è¾ƒå¤š ({old_data}/{total})\n"
                    "   å»ºè®®: æ›´æ–°ä¸ºæœ€æ–°å­£åº¦/å¹´åº¦æ•°æ®"
                )

        # å…è´£å£°æ˜
        if not has_disclaimer:
            self.recommendations.append(
                "âš ï¸ ç¼ºå°‘å…è´£å£°æ˜\n"
                "   å»ºè®®: åœ¨æ–‡æ¡£æœ«å°¾æ·»åŠ å…è´£å£°æ˜ï¼ˆæ³•å¾‹åˆè§„è¦æ±‚ï¼‰"
            )

    def print_report_v2(self, report: QualityReport):
        """æ‰“å°æŠ¥å‘Šv2.0ï¼ˆNEWï¼‰"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Šv2.0: {report.file_name}")
        print(f"{'='*80}")
        print(f"æ‰«ææ—¶é—´: {report.scan_time}")
        print(f"æ–‡ä»¶è·¯å¾„: {report.file_path}")
        print(f"{'='*80}\n")

        # è´¨é‡è¯„åˆ†ï¼ˆçªå‡ºæ˜¾ç¤ºï¼‰
        print("ğŸ¯ è´¨é‡è¯„åˆ†v2.0")
        print(f"  æ€»åˆ†: {report.quality_score}/100")
        print(f"  è¯„çº§: {report.quality_grade}")

        if report.quality_grade == 'A+':
            status = "ğŸŸ¢ å‘å¸ƒçº§ (90-100åˆ†)"
        elif report.quality_grade == 'B':
            status = "ğŸŸ¡ å¯ç”¨çº§ (70-89åˆ†)"
        elif report.quality_grade == 'C':
            status = "ğŸŸ  éœ€ä¿®è®¢ (50-69åˆ†)"
        else:
            status = "ğŸ”´ ä¸åˆæ ¼ (<50åˆ†)"
        print(f"  çŠ¶æ€: {status}\n")

        # è¯„åˆ†æ˜ç»†
        print("ğŸ“ˆ è¯„åˆ†æ˜ç»†")
        print(f"  å¯éªŒè¯æ•°æ®å æ¯”: {report.verifiable_data_ratio:.1f}% (æƒé‡40%)")
        print(f"  æ¥æºæ ‡æ³¨ç‡: {report.source_annotation_ratio:.1f}% (æƒé‡30%)")
        print(f"  æ—¶æ•ˆæ€§å¾—åˆ†: {report.timeliness_score:.1f}% (æƒé‡15%)")
        print(f"  é€»è¾‘å®Œæ•´æ€§: {report.logic_completeness_score:.1f}% (æƒé‡10%)")
        print(f"  å…è´£å£°æ˜: {'âœ… æœ‰' if report.disclaimer_present else 'âŒ æ— '} (æƒé‡5%)\n")

        # æ•°æ®ç»Ÿè®¡
        print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
        print(f"  æ€»æ•°æ®ç‚¹: {report.total_data_points}")
        print(f"  âœ… å¯éªŒè¯ (VERIFIED): {report.verified_count} "
              f"({report.verified_count/report.total_data_points*100:.1f}%)")
        print(f"  ğŸ“Š ä¼°ç®— (ESTIMATED): {report.estimated_count} "
              f"({report.estimated_count/report.total_data_points*100:.1f}%)")
        print(f"  âš ï¸ ç¤ºä¾‹ (SAMPLE): {report.sample_count} "
              f"({report.sample_count/report.total_data_points*100:.1f}%)")
        print(f"  âŒ æœªéªŒè¯: {report.unverified_count} "
              f"({report.unverified_count/report.total_data_points*100:.1f}%)\n")

        # æ¥æºåˆ†å±‚ï¼ˆNEWï¼‰
        print("ğŸ” æ•°æ®æ¥æºåˆ†å±‚")
        print(f"  Tier 1 (å®˜æ–¹/SEC): {report.source_tier1_count} "
              f"({report.source_tier1_count/report.total_data_points*100:.1f}%)")
        print(f"  Tier 2 (çŸ¥åæœºæ„): {report.source_tier2_count} "
              f"({report.source_tier2_count/report.total_data_points*100:.1f}%)")
        print(f"  Tier 3 (ç¤¾äº¤åª’ä½“): {report.source_tier3_count} "
              f"({report.source_tier3_count/report.total_data_points*100:.1f}%)")
        print(f"  æ— æ¥æº: {report.total_data_points - report.source_tier1_count - report.source_tier2_count - report.source_tier3_count}\n")

        # ä¸»è¦é—®é¢˜
        if report.issues:
            print(f"âš ï¸ ä¸»è¦é—®é¢˜ ({len(report.issues)}ä¸ª)")
            for i, issue in enumerate(report.issues[:10], 1):
                print(f"  {i}. {issue}")
            if len(report.issues) > 10:
                print(f"  ... è¿˜æœ‰{len(report.issues)-10}ä¸ªé—®é¢˜\n")

        # æ”¹è¿›å»ºè®®
        if report.recommendations:
            print("ğŸ’¡ æ”¹è¿›å»ºè®®")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"  {i}. {rec}\n")

        # è‡ªåŠ¨ä¿®æ­£å»ºè®®ï¼ˆNEWï¼‰
        if report.auto_fix_suggestions:
            print(f"ğŸ”§ è‡ªåŠ¨ä¿®æ­£å»ºè®® ({len(report.auto_fix_suggestions)}ä¸ª)")
            high_priority = [s for s in report.auto_fix_suggestions if s['severity'] == 'HIGH']
            if high_priority:
                print(f"  é«˜ä¼˜å…ˆçº§: {len(high_priority)}ä¸ª")
                for i, sug in enumerate(high_priority[:5], 1):
                    print(f"\n  [{i}] è¡Œ{sug['line']}: {sug['issue']}")
                    print(f"      {sug['suggestion'][:200]}...")
            else:
                print("  æš‚æ— é«˜ä¼˜å…ˆçº§é—®é¢˜\n")

        # ä½ç½®ä¿¡åº¦æ•°æ®
        low_confidence = sorted(
            [dp for dp in self.data_points if dp.confidence < 40],
            key=lambda x: x.confidence
        )
        if low_confidence:
            print(f"\nğŸ”´ ä½ç½®ä¿¡åº¦æ•°æ® (ç½®ä¿¡åº¦<40%, {len(low_confidence)}ä¸ª)")
            for i, dp in enumerate(low_confidence[:5], 1):
                print(f"  {i}. è¡Œ{dp.line_number} [{dp.label}] [{dp.source_tier}] "
                      f"ç½®ä¿¡åº¦{dp.confidence}%")
                print(f"     {dp.context[:80]}")

        print(f"\n{'='*80}\n")

    def export_json_v2(self, report: QualityReport, output_path: str):
        """å¯¼å‡ºJSONæŠ¥å‘Šv2.0"""
        data = {
            'report_version': 'v2.0',
            'file_name': report.file_name,
            'file_path': report.file_path,
            'scan_time': report.scan_time,
            'quality_summary': {
                'quality_score': report.quality_score,
                'quality_grade': report.quality_grade,
                'verifiable_data_ratio': report.verifiable_data_ratio,
                'source_annotation_ratio': report.source_annotation_ratio,
                'timeliness_score': report.timeliness_score,
                'logic_completeness_score': report.logic_completeness_score,
                'disclaimer_present': report.disclaimer_present
            },
            'data_statistics': {
                'total_data_points': report.total_data_points,
                'verified_count': report.verified_count,
                'estimated_count': report.estimated_count,
                'sample_count': report.sample_count,
                'unverified_count': report.unverified_count
            },
            'source_tier_statistics': {
                'tier1_count': report.source_tier1_count,
                'tier2_count': report.source_tier2_count,
                'tier3_count': report.source_tier3_count
            },
            'data_points': [
                {
                    'line': dp.line_number,
                    'value': dp.value,
                    'type': dp.data_type,
                    'label': dp.label,
                    'source_tier': dp.source_tier,
                    'confidence': dp.confidence,
                    'timestamp': dp.timestamp,
                    'context': dp.context[:100]
                }
                for dp in self.data_points
            ],
            'issues': report.issues,
            'recommendations': report.recommendations,
            'auto_fix_suggestions': report.auto_fix_suggestions
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… JSONæŠ¥å‘Šå·²å¯¼å‡º: {output_path}")

    def update_dashboard(self, dashboard_path: str):
        """æ›´æ–°è´¨é‡ä»ªè¡¨ç›˜ï¼ˆNEWï¼‰"""
        if not self.all_reports:
            return

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_files = len(self.all_reports)
        avg_score = sum(r.quality_score for r in self.all_reports) / total_files

        grade_counts = defaultdict(int)
        for r in self.all_reports:
            grade_counts[r.quality_grade] += 1

        alerts = [r for r in self.all_reports if r.quality_score < 50]

        # ç”Ÿæˆä»ªè¡¨ç›˜å†…å®¹
        dashboard_content = f"""# æ•°æ®è´¨é‡å®æ—¶ç›‘æ§ä»ªè¡¨ç›˜ v2.0

æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ€»ä½“è´¨é‡æ¦‚è§ˆ

- **å¹³å‡è¯„åˆ†**: {avg_score:.1f}/100
- **æ€»æ–‡ä»¶æ•°**: {total_files}
- **å‘Šè­¦æ•°é‡**: {len(alerts)} ä¸ªæ–‡ä»¶ä¸åˆæ ¼ (<50åˆ†)

### è¯„çº§åˆ†å¸ƒ
- A+ (90-100åˆ†): {grade_counts['A+']} ä¸ª
- B (70-89åˆ†): {grade_counts['B']} ä¸ª
- C (50-69åˆ†): {grade_counts['C']} ä¸ª
- F (<50åˆ†): {grade_counts['F']} ä¸ª

---

## ğŸ“ˆ å„æ–‡ä»¶è´¨é‡è¯„åˆ†

| æ–‡ä»¶ | è¯„åˆ† | è¯„çº§ | çŠ¶æ€ | å¯éªŒè¯ç‡ | æ¥æºæ ‡æ³¨ç‡ | ä¸»è¦é—®é¢˜ |
|------|------|------|------|----------|------------|----------|
"""

        for report in sorted(self.all_reports, key=lambda x: x.quality_score, reverse=True):
            status = "âœ… é€šè¿‡" if report.quality_score >= 70 else (
                "âš ï¸ éœ€ä¿®æ­£" if report.quality_score >= 50 else "âŒ ä¸åˆæ ¼"
            )
            main_issue = report.issues[0][:50] if report.issues else "-"

            dashboard_content += f"| {report.file_name} | {report.quality_score}/100 | {report.quality_grade} | {status} | {report.verifiable_data_ratio:.1f}% | {report.source_annotation_ratio:.1f}% | {main_issue} |\n"

        # æ·»åŠ å‘Šè­¦åŒºåŸŸ
        if alerts:
            dashboard_content += f"\n---\n\n## ğŸš¨ è´¨é‡å‘Šè­¦ ({len(alerts)}ä¸ª)\n\n"
            for alert in alerts:
                dashboard_content += f"### âŒ {alert.file_name}\n"
                dashboard_content += f"- è¯„åˆ†: {alert.quality_score}/100 (Fçº§)\n"
                dashboard_content += f"- å¯éªŒè¯æ•°æ®: {alert.verifiable_data_ratio:.1f}%\n"
                dashboard_content += f"- æ¥æºæ ‡æ³¨: {alert.source_annotation_ratio:.1f}%\n"
                dashboard_content += f"- ä¸»è¦é—®é¢˜:\n"
                for issue in alert.issues[:3]:
                    dashboard_content += f"  - {issue}\n"
                dashboard_content += "\n"

        # æ•°æ®è´¨é‡è¶‹åŠ¿
        total_points = sum(r.total_data_points for r in self.all_reports)
        total_verified = sum(r.verified_count for r in self.all_reports)
        total_tier1 = sum(r.source_tier1_count for r in self.all_reports)
        total_tier2 = sum(r.source_tier2_count for r in self.all_reports)

        dashboard_content += f"""---

## ğŸ“‰ æ•°æ®è´¨é‡è¶‹åŠ¿

- **æ€»æ•°æ®ç‚¹**: {total_points}
- **å¯éªŒè¯æ•°æ®**: {total_verified} ({total_verified/total_points*100:.1f}%)
- **Tier 1æ¥æº**: {total_tier1} ({total_tier1/total_points*100:.1f}%)
- **Tier 2æ¥æº**: {total_tier2} ({total_tier2/total_points*100:.1f}%)

---

## ğŸ¯ è´¨é‡æ”¹è¿›å»ºè®®

"""

        # æ±‡æ€»å»ºè®®
        if avg_score < 50:
            dashboard_content += "- ğŸ”´ **ç´§æ€¥**: æ•´ä½“æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œå»ºè®®æš‚åœå‘å¸ƒï¼Œå…¨é¢å®¡è®¡æ‰€æœ‰æ–‡ä»¶\n"
        elif avg_score < 70:
            dashboard_content += "- ğŸŸ¡ **æ³¨æ„**: æ•´ä½“æ•°æ®è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ä¼˜å…ˆä¿®æ­£Fçº§å’ŒCçº§æ–‡ä»¶\n"
        else:
            dashboard_content += "- ğŸŸ¢ **è‰¯å¥½**: æ•´ä½“æ•°æ®è´¨é‡è¾¾æ ‡ï¼Œç»§ç»­ä¿æŒ\n"

        if len(alerts) > 0:
            dashboard_content += f"- âš ï¸ è¯·ä¼˜å…ˆå¤„ç†{len(alerts)}ä¸ªä¸åˆæ ¼æ–‡ä»¶\n"

        dashboard_content += f"- ğŸ’¡ ç›®æ ‡: æ‰€æœ‰æ–‡ä»¶è¾¾åˆ°Bçº§(70åˆ†)ä»¥ä¸Š\n"

        # å†™å…¥æ–‡ä»¶
        with open(dashboard_path, 'w', encoding='utf-8') as f:
            f.write(dashboard_content)

        print(f"âœ… ä»ªè¡¨ç›˜å·²æ›´æ–°: {dashboard_path}")

    def export_fix_suggestions(self, output_path: str):
        """å¯¼å‡ºä¿®æ­£å»ºè®®æ–‡æ¡£ï¼ˆNEWï¼‰"""
        if not self.all_reports:
            return

        content = f"""# è‡ªåŠ¨ä¿®æ­£å»ºè®®æ±‡æ€»

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æœ¬æ–‡æ¡£åŒ…å«æ‰€æœ‰æ‰«ææ–‡ä»¶çš„è‡ªåŠ¨ä¿®æ­£å»ºè®®ï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºã€‚

---

"""

        # æ”¶é›†æ‰€æœ‰å»ºè®®å¹¶æ’åº
        all_suggestions = []
        for report in self.all_reports:
            for sug in report.auto_fix_suggestions:
                sug['file'] = report.file_name
                all_suggestions.append(sug)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        all_suggestions.sort(key=lambda x: (
            0 if x['severity'] == 'HIGH' else 1,
            x.get('line', 0)
        ))

        # åˆ†ç»„è¾“å‡º
        high_priority = [s for s in all_suggestions if s['severity'] == 'HIGH']
        medium_priority = [s for s in all_suggestions if s['severity'] == 'MEDIUM']

        content += f"## ğŸ”´ é«˜ä¼˜å…ˆçº§é—®é¢˜ ({len(high_priority)}ä¸ª)\n\n"
        for i, sug in enumerate(high_priority, 1):
            content += f"### [{i}] {sug['file']} - è¡Œ{sug['line']}\n\n"
            content += f"**é—®é¢˜**: {sug['issue']}\n\n"
            content += f"**ä¿®æ­£å»ºè®®**:\n```\n{sug['suggestion']}\n```\n\n"
            content += "---\n\n"

        content += f"## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§é—®é¢˜ ({len(medium_priority)}ä¸ª)\n\n"
        for i, sug in enumerate(medium_priority, 1):
            content += f"### [{i}] {sug['file']} - è¡Œ{sug['line']}\n\n"
            content += f"**é—®é¢˜**: {sug['issue']}\n\n"
            content += f"**ä¿®æ­£å»ºè®®**:\n```\n{sug['suggestion']}\n```\n\n"
            content += "---\n\n"

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ… ä¿®æ­£å»ºè®®å·²å¯¼å‡º: {output_path}")


class FileChangeHandler(FileSystemEventHandler):
    """æ–‡ä»¶å˜åŒ–ç›‘å¬å™¨ï¼ˆNEWï¼‰"""

    def __init__(self, monitor: DataIntegrityMonitorV2, dashboard_path: str):
        self.monitor = monitor
        self.dashboard_path = dashboard_path
        self.last_scan = {}

    def on_modified(self, event):
        if event.is_directory:
            return

        file_path = event.src_path
        if not file_path.endswith('.md'):
            return

        # é¿å…é‡å¤æ‰«æï¼ˆ1åˆ†é’Ÿå†…ï¼‰
        now = time.time()
        if file_path in self.last_scan:
            if now - self.last_scan[file_path] < 60:
                return

        self.last_scan[file_path] = now

        print(f"\nğŸ”” æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–: {file_path}")
        self.scan_and_update(file_path)

    def on_created(self, event):
        if event.is_directory:
            return

        file_path = event.src_path
        if not file_path.endswith('.md'):
            return

        print(f"\nğŸ”” æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {file_path}")
        time.sleep(1)  # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
        self.scan_and_update(file_path)

    def scan_and_update(self, file_path: str):
        """æ‰«ææ–‡ä»¶å¹¶æ›´æ–°ä»ªè¡¨ç›˜"""
        try:
            report = self.monitor.scan_file(file_path)
            if report:
                self.monitor.print_report_v2(report)

                # æ›´æ–°æŠ¥å‘Šåˆ—è¡¨
                # ç§»é™¤æ—§æŠ¥å‘Šï¼ˆåŒæ–‡ä»¶ï¼‰
                self.monitor.all_reports = [
                    r for r in self.monitor.all_reports
                    if r.file_path != file_path
                ]
                self.monitor.all_reports.append(report)

                # å¯¼å‡ºJSON
                json_path = file_path.replace('.md', '_quality_report_v2.json')
                self.monitor.export_json_v2(report, json_path)

                # æ›´æ–°ä»ªè¡¨ç›˜
                self.monitor.update_dashboard(self.dashboard_path)

                # å‘Šè­¦
                if report.quality_score < 50:
                    print(f"\nğŸš¨ ã€è´¨é‡å‘Šè­¦ã€‘ {report.file_name} è¯„åˆ†è¿‡ä½: {report.quality_score}/100")
                    print(f"   å»ºè®®: ç«‹å³ä¿®æ­£æˆ–åˆ é™¤æ­¤æ–‡ä»¶\n")

        except Exception as e:
            print(f"âŒ æ‰«æå¤±è´¥: {e}")


def watch_directory(watch_path: str, dashboard_path: str):
    """å®æ—¶ç›‘æ§ç›®å½•ï¼ˆNEWï¼‰"""
    if not WATCHDOG_AVAILABLE:
        print("âŒ watchdogæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ç›‘æ§åŠŸèƒ½")
        print("å®‰è£…å‘½ä»¤: pip install watchdog")
        return

    monitor = DataIntegrityMonitorV2()

    # åˆå§‹æ‰«æ
    print(f"ğŸ“ å¼€å§‹åˆå§‹æ‰«æ: {watch_path}")
    md_files = list(Path(watch_path).glob('*.md'))

    for md_file in md_files:
        try:
            report = monitor.scan_file(str(md_file))
            if report:
                monitor.all_reports.append(report)
                # å¯¼å‡ºJSON
                json_path = str(md_file).replace('.md', '_quality_report_v2.json')
                monitor.export_json_v2(report, json_path)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {md_file}: {e}")

    # ç”Ÿæˆåˆå§‹ä»ªè¡¨ç›˜
    monitor.update_dashboard(dashboard_path)

    # å¯¼å‡ºä¿®æ­£å»ºè®®
    suggestions_path = os.path.join(
        os.path.dirname(dashboard_path),
        'Auto_Fix_Suggestions.md'
    )
    monitor.export_fix_suggestions(suggestions_path)

    print(f"\nâœ… åˆå§‹æ‰«æå®Œæˆ: {len(monitor.all_reports)} ä¸ªæ–‡ä»¶")
    print(f"âœ… ä»ªè¡¨ç›˜: {dashboard_path}")
    print(f"âœ… ä¿®æ­£å»ºè®®: {suggestions_path}")

    # å¼€å§‹å®æ—¶ç›‘æ§
    event_handler = FileChangeHandler(monitor, dashboard_path)
    observer = Observer()
    observer.schedule(event_handler, watch_path, recursive=False)
    observer.start()

    print(f"\nğŸ‘€ å¼€å§‹å®æ—¶ç›‘æ§: {watch_path}")
    print("   (æŒ‰ Ctrl+C åœæ­¢)\n")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
        print("\nğŸ›‘ ç›‘æ§å·²åœæ­¢")

    observer.join()


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("""
æ•°æ®å®Œæ•´æ€§ç›‘æµ‹å·¥å…· v2.0
========================

ä½¿ç”¨æ–¹æ³•:
  1. å•æ–‡ä»¶æ‰«æ:
     python data_integrity_monitor_v2.0.py <æ–‡ä»¶è·¯å¾„>

  2. å®æ—¶ç›‘æ§æ¨¡å¼:
     python data_integrity_monitor_v2.0.py --watch
     python data_integrity_monitor_v2.0.py --watch --path <ç›®å½•è·¯å¾„>

  3. ç›‘æ§+è‡ªåŠ¨æ›´æ–°ä»ªè¡¨ç›˜:
     python data_integrity_monitor_v2.0.py --watch --auto-update-dashboard

ç¤ºä¾‹:
  python data_integrity_monitor_v2.0.py Tesla_Report.md
  python data_integrity_monitor_v2.0.py --watch
  python data_integrity_monitor_v2.0.py --watch --path ./IntelligenceEngine_v10

æ–°åŠŸèƒ½ (v2.0):
  âœ… å®æ—¶æ–‡ä»¶ç›‘æ§
  âœ… è´¨é‡è¯„åˆ†ç®—æ³•v2.0 (A+/B/C/Fè¯„çº§)
  âœ… æ•°æ®æ¥æºåˆ†å±‚ (Tier 1/2/3)
  âœ… è‡ªåŠ¨ä¿®æ­£å»ºè®®ç”Ÿæˆ
  âœ… å®æ—¶è´¨é‡ä»ªè¡¨ç›˜
  âœ… è´¨é‡<50åˆ†è‡ªåŠ¨å‘Šè­¦
  âœ… æ•°æ®æ—¶æ•ˆæ€§è¯„åˆ†
""")
        sys.exit(1)

    # ç›‘æ§æ¨¡å¼
    if '--watch' in sys.argv:
        # ç¡®å®šç›‘æ§è·¯å¾„
        if '--path' in sys.argv:
            path_index = sys.argv.index('--path') + 1
            watch_path = sys.argv[path_index] if path_index < len(sys.argv) else '.'
        else:
            watch_path = '/Users/milton/æŠ•èµ„å¤§å¸ˆ'

        # ä»ªè¡¨ç›˜è·¯å¾„
        dashboard_path = os.path.join(watch_path, 'Quality_Dashboard.md')

        watch_directory(watch_path, dashboard_path)

    # å•æ–‡ä»¶æ¨¡å¼
    else:
        file_path = sys.argv[1]

        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
            sys.exit(1)

        monitor = DataIntegrityMonitorV2()
        report = monitor.scan_file(file_path)

        if report:
            monitor.print_report_v2(report)

            # å¯¼å‡ºJSON
            json_path = file_path.replace('.md', '_quality_report_v2.json')
            monitor.export_json_v2(report, json_path)

            # å¦‚æœè¯„åˆ†è¿‡ä½ï¼Œç”Ÿæˆä¿®æ­£å»ºè®®
            if report.quality_score < 70:
                fix_path = file_path.replace('.md', '_fix_suggestions.md')
                monitor.all_reports = [report]
                monitor.export_fix_suggestions(fix_path)
                print(f"âœ… ä¿®æ­£å»ºè®®å·²å¯¼å‡º: {fix_path}")


if __name__ == '__main__':
    main()
