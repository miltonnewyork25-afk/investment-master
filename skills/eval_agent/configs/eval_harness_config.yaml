# Eval Harness Config v1.3
# Runner/Recorder/Grader 模块化配置

harness:
  version: "1.3"
  mode: "production"  # production | debug | dry_run

---

# Runner配置
runner:
  id: "eval_harness_runner"

  execution:
    strategy: "fail_fast_on_safety"
    parallelism:
      max_concurrent_suites: 3
      max_concurrent_cases: 10
      thread_pool_size: 20

    timeouts:
      suite_default_ms: 300000
      case_default_ms: 30000
      total_run_ms: 600000

    retry:
      enabled: true
      max_retries: 2
      backoff_strategy: "exponential"
      initial_delay_ms: 1000

  isolation:
    enabled: true
    sandbox_mode: "container"  # container | process | none
    resource_limits:
      memory_mb: 4096
      cpu_cores: 2
      network: "restricted"

  logging:
    level: "INFO"
    include_inputs: true
    include_outputs: true
    redact_pii: true
    format: "json"

---

# Recorder配置
recorder:
  id: "eval_trace_recorder"

  storage:
    backend: "file"  # file | database | s3
    path: "eval_traces/{run_id}/"
    format: "jsonl"
    compression: "gzip"
    retention_days: 365

  trace_schema:
    version: "1.3"
    required_fields:
      - run_id
      - suite_id
      - case_id
      - input
      - expected_output
      - actual_output
      - scores
      - latency_ms
      - token_usage
      - timestamp

  indexing:
    enabled: true
    fields:
      - run_id
      - suite_id
      - verdict
      - timestamp

  streaming:
    enabled: true
    buffer_size: 100
    flush_interval_ms: 5000

---

# Grader配置
graders:
  # 规则Grader配置
  rule_graders:
    id: "eval_rule_graders"

    types:
      exact_match:
        id: "grader_exact_match"
        config:
          case_sensitive: false
          whitespace_normalize: true
          encoding: "utf-8"

      numeric_range:
        id: "grader_numeric_range"
        config:
          tolerance_type: "absolute"  # absolute | relative | percentage
          default_tolerance: 0.01
          nan_handling: "fail"
          inf_handling: "fail"

      schema_validation:
        id: "grader_schema_validation"
        config:
          schema_version: "draft-07"
          strict_mode: true
          additional_properties: false

      regex_pattern:
        id: "grader_regex_pattern"
        config:
          flavor: "python"
          flags: ["IGNORECASE", "MULTILINE"]
          match_type: "search"  # search | match | fullmatch

      semantic_hash:
        id: "grader_semantic_hash"
        config:
          algorithm: "simhash"
          hash_bits: 64
          threshold: 0.95

      list_containment:
        id: "grader_list_containment"
        config:
          order_sensitive: false
          allow_subset: true
          min_overlap: 0.8

      tool_call_sequence:
        id: "grader_tool_sequence"
        config:
          strict_order: true
          allow_extra_calls: false
          param_validation: "schema"

    aggregation:
      strategy: "weighted_average"
      pass_threshold: 0.85
      weights:
        exact_match: 1.0
        schema_validation: 1.0
        tool_call_sequence: 1.5  # 工具调用更重要

  # LLM Judge配置
  llm_judges:
    id: "eval_llm_judges"

    constraints:
      can_block: false
      require_human_fallback: true
      allowed_suites: ["golden_outcome", "metamorphic"]

    model:
      provider: "anthropic"
      model_id: "claude-3-5-sonnet-20241022"
      temperature: 0.0
      max_tokens: 1000

    prompts:
      semantic_equivalence:
        system: |
          You are an expert evaluator assessing semantic equivalence between two texts.
          Score from 0.0 to 1.0 where 1.0 means perfectly equivalent meaning.
          Focus on meaning, not exact wording.
        user_template: |
          Expected: {expected}
          Actual: {actual}

          Provide a score (0.0-1.0) and brief rationale.

      reasoning_quality:
        system: |
          You are an expert evaluator assessing reasoning quality.
          Score from 0.0 to 1.0 based on logical coherence, evidence usage, and conclusion validity.
        user_template: |
          Reasoning: {reasoning}
          Context: {context}

          Provide a score (0.0-1.0) and brief rationale.

      consistency_check:
        system: |
          You are an expert evaluator assessing consistency between two outputs.
          Score from 0.0 to 1.0 where 1.0 means perfectly consistent.
        user_template: |
          Output A: {output_a}
          Output B: {output_b}
          Transform applied: {transform}

          Provide a score (0.0-1.0) and brief rationale.

    output_format:
      type: "structured"
      schema:
        score:
          type: "number"
          minimum: 0.0
          maximum: 1.0
        rationale:
          type: "string"
          max_length: 500
        confidence:
          type: "number"
          minimum: 0.0
          maximum: 1.0

    fallback:
      enabled: true
      trigger_conditions:
        - confidence_below: 0.6
        - score_variance_above: 0.2
      action: "human_review"

---

# Case Generator配置
case_generator:
  id: "eval_case_generator"

  from_decision_trace:
    enabled: true

    extraction:
      input_fields: ["artifacts", "claim_registry", "evidence_registry"]
      output_fields: ["safe_output", "verdict", "reason_codes"]
      metadata_fields: ["run_id", "policy_pack_version", "stats"]

    transformation:
      normalize_input: true
      redact_pii: true
      anonymize_ids: false

    deduplication:
      enabled: true
      algorithm: "simhash"
      threshold: 0.9
      scope: "last_30_days"

  case_format:
    version: "1.3"
    schema:
      id:
        type: "string"
        format: "uuid"
      suite:
        type: "string"
        enum: ["canary", "safety", "golden_outcome", "tool_precision", "governance", "metamorphic"]
      input:
        type: "object"
      expected_output:
        type: "object"
      grader_config:
        type: "object"
      metadata:
        type: "object"
        properties:
          source: { type: "string" }
          priority: { type: "string", enum: ["P0", "P1", "P2"] }
          created_at: { type: "string", format: "date-time" }
          tags: { type: "array", items: { type: "string" } }

---

# Drift Detector配置
drift_detector:
  id: "eval_drift_detector"

  metrics:
    - name: "task_success_rate"
      type: "ratio"
      aggregation: "mean"
      window_runs: 10

    - name: "avg_latency_ms"
      type: "numeric"
      aggregation: "p50"
      window_runs: 10

    - name: "token_efficiency"
      type: "ratio"
      aggregation: "mean"
      window_runs: 10

    - name: "safety_score"
      type: "ratio"
      aggregation: "min"
      window_runs: 5

  detection:
    algorithm: "statistical"
    sigma_threshold: 2.0
    min_samples: 5

    methods:
      z_score:
        enabled: true
        threshold: 2.0

      mann_whitney:
        enabled: true
        p_value_threshold: 0.05

      trend_analysis:
        enabled: true
        consecutive_threshold: 3

  alerting:
    enabled: true
    channels:
      - type: "log"
        level: "WARNING"
      - type: "webhook"
        url: "${DRIFT_ALERT_WEBHOOK}"

    throttle:
      window_minutes: 60
      max_alerts: 3

---

# Flakiness Detector配置
flakiness_detector:
  id: "eval_flakiness_detector"

  detection:
    min_runs: 5
    metrics:
      - name: "pass_fail_variance"
        threshold: 0.15
      - name: "score_coefficient_of_variation"
        threshold: 0.20
      - name: "latency_variance"
        threshold: 0.30

  classification:
    stable:
      variance_below: 0.05
    unstable:
      variance_between: [0.05, 0.15]
    flaky:
      variance_between: [0.15, 0.25]
    very_flaky:
      variance_above: 0.25

  actions:
    quarantine:
      enabled: true
      threshold: "flaky"
      duration_days: 7
      auto_retest: true

    report:
      include_in_scorecard: true
      flag_in_results: true

    retry:
      strategy: "selective"
      max_retries: 3
      only_for: ["unstable", "flaky"]

---

# 输出配置
output:
  scorecard:
    format: "json"
    include_details: true
    include_raw_scores: false

  regression_report:
    format: "markdown"
    template: "default"
    include_charts: false

  release_verdict:
    format: "json"
    include_rationale: true
    include_blocking_reasons: true

---

# 版本历史
version_history:
  - version: "1.3"
    date: "2026-01-27"
    changes:
      - "模块化Grader配置"
      - "Case Generator配置"
      - "Drift Detector配置"
      - "Flakiness Detector配置"
      - "LLM Judge约束强化"
