# Learning Sentinel 提案 — 规划阶段报告学习哨兵

> **状态**: 提案 (待master设计)
> **来源**: 生态科技worktree用户反馈 (2026-02-14)
> **优先级**: 高 — 填补飞轮断裂环节

---

## 1. 问题诊断

当前复利飞轮存在一个断裂环节：

```
做完报告 → 事后反思 → 编码教训到docs/ → ??? → 下一份报告启动
                                           ↑
                                      这里断了
```

**教训写了，但下次没人系统性消化。** 具体表现：

- `compound_learning_flywheel.md` 有16条实证教训，但Phase 0/0.5没有Agent去读取和应用
- `find_best_reference.sh` (铁律H) 只找到最佳版本，没有深度提取可复用模式
- Agent A/B/C v7.1 是执行阶段身份，规划阶段没有专门的学习角色
- 每次新报告启动时，框架知识靠CLAUDE.md被动加载，不是主动从优秀报告中提取

**结果**: 报告质量提升依赖框架升级(被动)，而非从最佳实践中主动学习(主动)。

---

## 2. 提案概述

在 Phase 0/0.5 规划阶段，新增一个 **Learning Sentinel** 角色：

- **触发时机**: Phase 0.5 开始前，用户提供优秀报告作为参考
- **输入**: 用户指定的1-3份优秀报告(可以是本框架产出的，也可以是外部优秀研报)
- **输出**: 结构化的"参考基准文件"，供后续Phase的Agent A/B/C消费
- **定位**: 不是第四个执行Agent，而是规划阶段的一次性学习任务

---

## 3. 待master设计的核心问题

### 3.1 身份定义 (v7.1格式)

需要定义：
- **identity**: 这个角色的分析哲学是什么？（对标Agent A商业洞察/B风险审计/C定量估值）
- **quality_bar**: 它应该提取什么层次的洞察？（结构模式？叙事技巧？数据深度？章节编排？）
- **this_session**: 编排器如何每次填入具体任务？

### 3.2 输出格式

产出什么文件？建议方向：
- `reports/{TICKER}/data/learning_baseline.md` — 从优秀报告中提取的参考基准
- 内容可能包括：
  - 章节结构模式（什么顺序、什么粒度效果最好）
  - 叙事技巧（如何将数据转化为洞察的典型范式）
  - 数据深度标杆（标注密度、Mermaid使用、DM锚点分布）
  - 非共识洞察的发现路径（CI是怎么被挖出来的）
  - 估值方法的组合方式（方法离散度如何处理）
  - **反面教训**: 优秀报告中仍然存在的弱点/可改进之处

### 3.3 与现有组件的关系

| 现有组件 | Learning Sentinel如何与之协作 |
|---------|---------------------------|
| 铁律H + find_best_reference.sh | Sentinel在其基础上深读，不仅"找到"还要"消化" |
| compound_learning_flywheel.md | Sentinel消费飞轮的历史教训，不重复提取 |
| Phase 0.5 CQ提取 | Sentinel产出可与CQ并行，两者独立但互补 |
| Agent A/B/C shared_context.md | Sentinel的产出写入shared_context或独立文件，供A/B/C参考 |
| quality_sentinel.sh | 不同角色：quality_sentinel检查产出质量，learning_sentinel提取学习模式 |

### 3.4 适用范围

- **强制触发**: 用户提供了参考报告时
- **可选触发**: 同行业有已完成的优秀报告时（自动推荐）
- **跳过条件**: 首份行业报告、无参考可用时

### 3.5 开放问题

1. **Sentinel是独立Agent还是编排器的一个步骤？** — 如果报告很长(400K+)，需要独立Agent处理
2. **如何避免"模仿"而非"学习"？** — 提取模式≠复制结构，需要特异性过滤
3. **跨行业参考的处理？** — 如用半导体LRCX的结构去分析生态科技MSFT，哪些可迁移？
4. **与v12.0 DAG编排器的关系？** — 如果DAG在未来验证，Sentinel是DAG-0的一个节点还是独立？
5. **外部报告的处理？** — 用户可能提供非本框架的优秀研报，格式完全不同

---

## 4. 实证依据

从已有10份报告的经验来看，学习效果最好的场景：

| 场景 | 学习来源 | 效果 |
|------|---------|------|
| GOOGL v4.0 | 参考AMD v9.0教训 | 密度从33→44/万，标注质量飞跃 |
| TSLA v3.0 | 参考LRCX v9.0框架 | 首次完整落地零仓位建议 |
| PLTR v3.1 | 参考GOOGL v4.0结构 | DM锚点从自由文本→结构化 |
| AMD v2.0 | 参考LRCX Agent配置 | 首次3Agent P5铁律严格执行 |

**规律**: 每次"学习前作"都显著提升了下一份报告的质量。但这些学习都是隐式的（靠MEMORY.md和用户提醒），不是系统性的。

---

## 5. 建议设计方向

**最小可行版本 (MVP)**:
1. 用户提供报告路径
2. Sentinel读取报告，产出结构化的 `learning_baseline.md`
3. 该文件在Phase 0.5后写入 `shared_context.md` 或独立文件
4. Agent A/B/C在各自prompt中被告知参考该文件

**进阶版本**:
- 自动对比当前报告与参考报告的差距（实时）
- 在每个Phase完成后，Sentinel检查产出是否达到参考基准
- 与quality_sentinel.sh集成，自动评估"学到了多少"

---

*等待master设计完整方案后，同步到所有worktree。*
*用户将提供优秀报告作为设计输入。*
